Create Lambda function
aws lambda create-function \
  --function-name MyLambdaFunction \
  --runtime python3.8 \
  --role arn:aws:iam::your-account-id:role/execution_role \
  --handler index.handler \
  --zip-file fileb:///path/to/your/lambda_function.zip

Streaming body
  response = s3.get_object(Bucket=bucket_name, Key=file_key)
    with io.TextIOWrapper(response['Body'], encoding='utf-8') as f:
        while True:
            chunk = f.readlines(chunk_size)
            if not chunk:
                global flag
                flag = False
                break  # Exit the loop if the chunk is empty (end of file)
            yield chunk

Check if ecs task login is enabled
bash <( curl -Ls https://raw.githubusercontent.com/aws-containers/amazon-ecs-exec-checker/main/check-ecs-exec.sh )
 cluster_name_place_holder task-arn

Start session inside task image
 aws ssm start-session --target task-arn_place-holder


 Identify the Linux Distribution
 cat /etc/os-release

import json
import boto3

def lambda_handler(event, context):
    # Set your ECS cluster name
    cluster_name = 'DevCluster'
    
    subnet_id = 'subnet-'
    security_group_ids = ['sg-']

    # Set your ECS task definition ARN
    task_definition_arn = 'arn:aws:ecs:us-east-1:604830111080:task-definition/myfastapiapp:1'

   
    ecs_client = boto3.client('ecs')

    response = ecs_client.run_task(
        cluster=cluster_name,
        taskDefinition=task_definition_arn,
        launchType='FARGATE',
        networkConfiguration={
            'awsvpcConfiguration': {
                'subnets': [subnet_id],
                'securityGroups': security_group_ids,
                'assignPublicIp': 'ENABLED'
            }
        }
    )

    print(response)


    import os
import boto3

def lambda_handler(event, context):
    # Replace these values with your own S3 bucket and file information
    bucket_name = 'your-s3-bucket-name'
    key = 'path/to/your/file.txt'
    local_file_path = '/tmp/your-file.txt'  # Lambda has limited access to local storage, use /tmp directory

    try:
        # Create an S3 client
        s3 = boto3.client('s3')

        # Download the file from S3 to a local file in /tmp directory
        s3.download_file(bucket_name, key, local_file_path)
        print(f'Successfully downloaded file from S3 to {local_file_path}')

        # Check the size of the downloaded file
        file_size = os.path.getsize(local_file_path)
        print(f'Downloaded file size: {file_size} bytes')

        # Perform additional processing with the downloaded file if needed

        return {
            'statusCode': 200,
            'body': 'File downloaded successfully'
        }

    except Exception as e:
        print(f'Error downloading file: {e}')
        return {
            'statusCode': 500,
            'body': 'Error downloading file'
        }

#build image
docker build -t my-python-app .

#run image
docker run -p 4000:80 my-python-app


import time
import threading

exc = True

def count_up_to_200():
    counter = 0
    # for i in range(201):
    #     print(f"Counting up: {i}")
    for i in range(201):
        if not exc:
            break
        print(f"Counting up: {i}")
        time.sleep(0.1)  # Simulating some work
    print("Count up complete!")

def count_down_from_200():
    for i in range(50, -1, -1):
        print(f"Counting down: {i}")
        time.sleep(0.1)  # Simulating some work
    global exc
    exc = False
    print("Count down complete!")

if __name__ == "__main__":
    # Create threads for parallel execution
    count_up_thread = threading.Thread(target=count_up_to_200)
    count_down_thread = threading.Thread(target=count_down_from_200)

    # Start the threads
    count_up_thread.start()
    count_down_thread.start()

    # Wait for both threads to finish
    count_up_thread.join()
    count_down_thread.join()

    print('Main thread exiting')


# Create a cursor
cursor = conn.cursor()

# Specify the schema and table name
schema_name = "your_schema_name"
table_name = "calendar"

# Get the current year
current_year = datetime.now().year

# Define the columns you want to select
selected_columns = ["column1", "column2", "column3", "column4"]

# Construct the SQL query with schema, table name, and conditions
sql_query = f"SELECT {', '.join(selected_columns)} FROM {schema_name}.{table_name} WHERE year = {current_year} AND business_flag = 'Y' AND holiday = 'N';"

# Execute the query
cursor.execute(sql_query)

# Fetch all the rows
result = cursor.fetchall()

# Print or process the result as needed
for row in result:
    print(row)

# Close the cursor and connection
cursor.close()
conn.close()

# Use an official Python runtime as a parent image
FROM python:3.8

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Make port 80 available to the world outside this container
EXPOSE 80

# Define environment variable
ENV NAME World

# Run script.py when the container launches
CMD ["python", "main.py"]


FROM python:3.9-slim

WORKDIR /app

COPY . /app

RUN pip install --no-cache-dir -r requirements.txt

RUN apt-get update && apt-get install -y nano

EXPOSE 80

ENV NAME world

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80", "--reload"]

docker run -e AWS_ACCESS_KEY_ID=your_access_key -e AWS_SECRET_ACCESS_KEY=your_secret_key your_image_name


import subprocess
import json

def lambda_handler(event, context):
    # Set your AWS CLI command
    aws_cli_command = "aws ecs run-task --cluster YourEcsClusterName --task-definition YourTaskDefinition"

    try:
        # Run the AWS CLI command using subprocess
        result = subprocess.run(aws_cli_command, shell=True, check=True, capture_output=True, text=True)

        # Print the output of the command (optional)
        print(result.stdout)

        # You can also parse the JSON output if needed
        output_json = json.loads(result.stdout)
        # Do something with the output JSON

        return {
            'statusCode': 200,
            'body': json.dumps('ECS task successfully started!')
        }

    except subprocess.CalledProcessError as e:
        # Handle any errors that occurred during the subprocess execution
        print(f"Error: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {e}')
        }

# Note: Replace "YourEcsClusterName" and "YourTaskDefinition" with your actual ECS cluster name and task definition ARN.

aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')


import boto3

def stop_fargate_task(cluster_name, task_id):
    # Initialize ECS client
    ecs_client = boto3.client('ecs')

    # Stop the Fargate task
    try:
        response = ecs_client.stop_task(
            cluster=cluster_name,
            task=task_id
        )
        print(f"Task {task_id} stopped successfully.")
    except Exception as e:
        print(f"Error stopping task {task_id}: {str(e)}")

if __name__ == "__main__":
    # Replace 'your_cluster_name' and 'your_task_id' with your actual values
    cluster_name = 'your_cluster_name'
    task_id = 'your_task_id'

    stop_fargate_task(cluster_name, task_id)

